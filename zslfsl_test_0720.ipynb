{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2821ccf",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:no conf file\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_539/1674242798.py\", line 469, in <module>\n",
      "    main(confpath, output_path, logger=logger)\n",
      "  File \"/tmp/ipykernel_539/1674242798.py\", line 322, in main\n",
      "    raise Exception('no conf file')\n",
      "Exception: no conf file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import shutil\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import datetime\n",
    "import torch\n",
    "from numpy.lib.utils import source\n",
    "import pandas as pd\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from sklearn import metrics\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from flair.models.text_classification_model import TARSClassifier\n",
    "from flair.data import Sentence, Corpus\n",
    "from flair.datasets import SentenceDataset\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, SentenceTransformerDocumentEmbeddings, TransformerDocumentEmbeddings\n",
    "from flair.data import Sentence\n",
    "\n",
    "\n",
    "def vectorize(sentences, embed_type='sentencetransformer'):\n",
    "    # TODO word / document embedding, check results\n",
    "    \n",
    "    if embed_type == 'sentencetransformer':\n",
    "        embedding = SentenceTransformerDocumentEmbeddings('bert-base-nli-mean-tokens')\n",
    "    elif embed_type == 'roberta':\n",
    "        embedding = TransformerDocumentEmbeddings('roberta-base')\n",
    "    else:\n",
    "        raise Exception('unknown model - ' + embed_type)\n",
    "    \n",
    "    try:\n",
    "        vectors = []\n",
    "        for st in sentences:\n",
    "            st = Sentence(st)\n",
    "\n",
    "            embedding.embed(st)\n",
    "            vec = st.embedding\n",
    "            vectors.append(vec)\n",
    "\n",
    "    except Exception as e:\n",
    "        # change pretrained model if fails\n",
    "        embedding = TransformerDocumentEmbeddings('roberta-base')\n",
    "        vectors = []\n",
    "        for st in sentences:\n",
    "            st = Sentence(st)\n",
    "\n",
    "            embedding.embed(st)\n",
    "            vec = st.embedding\n",
    "            vectors.append(vec)\n",
    "\n",
    "    vectors = torch.stack(vectors)\n",
    "    vectors = tensor_to_array(vectors)\n",
    "    \n",
    "    return vectors\n",
    "\n",
    "\n",
    "def tensor_to_array(vectors):\n",
    "    if torch.is_tensor(vectors):\n",
    "        if vectors.requires_grad:\n",
    "            vectors = vectors.detach()\n",
    "        if vectors.is_cuda:\n",
    "            vectors = vectors.cpu()\n",
    "        vectors = vectors.numpy()\n",
    "    return vectors\n",
    "\n",
    "\n",
    "def load(filepath):\n",
    "    with open(filepath, 'rt') as f:\n",
    "        if filepath.endswith('.csv'):\n",
    "            df = pd.read_csv(filepath)\n",
    "        elif filepath.endswith('.tsv'):\n",
    "            df = pd.read_table(filepath)\n",
    "        elif filepath.endswith('.xls') or filepath.endswith('.xlsx'):\n",
    "            df = pd.read_excel(filepath)\n",
    "        elif filepath.endswith('.txt'):\n",
    "            texts = [line.strip() for line in f.readlines()]\n",
    "            df = pd.DataFrame(texts, columns=['text'])\n",
    "        elif filepath.endswith('.json'):\n",
    "            df = pd.read_json(f, orient='table')\n",
    "        elif filepath.endswith('.pkl'):\n",
    "            df = pickle.load(f)\n",
    "        else:\n",
    "            raise Exception('unknown extension')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_output_file(filename, obj):\n",
    "    workspace = Path(os.environ.get('ACCUTUNING_WORKSPACE'))\n",
    "    output_path = Path(workspace, 'output')\n",
    "    fp = pathlib.Path(output_path, filename)\n",
    "    fp.write_bytes(\n",
    "        pickle.dumps(obj)\n",
    "    )\n",
    "    return str(fp.relative_to(workspace))\n",
    "\n",
    "\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "def save_output_file(filename, obj):\n",
    "    workspace = Path(os.environ.get('ACCUTUNING_WORKSPACE'))\n",
    "    output_path = Path(workspace, 'output')\n",
    "    fp = pathlib.Path(output_path, filename)\n",
    "    fp.write_bytes(\n",
    "        pickle.dumps(obj)\n",
    "    )\n",
    "    logger.critical('Saved output file - {}'.format(filename))\n",
    "    return str(fp.relative_to(workspace))\n",
    "\n",
    "\n",
    "def open_output_file(filename, none_if_not_exist=False):\n",
    "    workspace = Path(os.environ.get('ACCUTUNING_WORKSPACE'))\n",
    "    output_path = Path(workspace, 'output')\n",
    "    fp = pathlib.Path(\n",
    "        output_path,\n",
    "        filename\n",
    "    )\n",
    "    if fp.exists():\n",
    "        pass\n",
    "    else:\n",
    "        if none_if_not_exist:\n",
    "            return None\n",
    "        raise Exception('{} does not exist'.format(fp))\n",
    "\n",
    "    logger.critical('Open output file - {}'.format(filename))\n",
    "    return pickle.loads(fp.read_bytes())\n",
    "\n",
    "\n",
    "def zsl(\n",
    "    data,\n",
    "    target_column_nm,\n",
    "    class_nm_list\n",
    "):\n",
    "    ret = {}\n",
    "\n",
    "    logger.critical('Start ZSL')\n",
    "    start_t =datetime.datetime.now()\n",
    "\n",
    "    texts = data[target_column_nm].values.tolist()\n",
    "\n",
    "    #############################################################################\n",
    "    # vector for sentences\n",
    "    logger.critical('ZSL - vectorizing texts')\n",
    "    sentences = []\n",
    "    vectors = vectorize(texts)\n",
    "    for i in range(0, len(texts)):\n",
    "        sentence = Sentence(texts[i])\n",
    "        sentences.append(sentence)\n",
    "    ret['vectors'] = save_output_file(\n",
    "        'vectors.pkl',\n",
    "        vectors\n",
    "    )\n",
    "\n",
    "    logger.critical('ZSL - loading pre-trained model')\n",
    "    tars = TARSClassifier.load('./resources/taggers/agnews_all/final-model.pt') # Load model\n",
    "\n",
    "    logger.critical('ZSL - predicting zero shot')\n",
    "    predictions = []\n",
    "    for sentence in sentences:\n",
    "        tars.predict_zero_shot(sentence, class_nm_list)\n",
    "        number_list = [c['confidence'] for c in sentence.to_dict()['labels']]\n",
    "        if len(number_list) >  0:\n",
    "            max_value = max(number_list)\n",
    "            max_index = number_list.index(max_value) \n",
    "            predictions.append(sentence.to_dict()['labels'][max_index]['value'])\n",
    "        else:\n",
    "            predictions.append('분류못함')\n",
    "\n",
    "    ret['labels'] = save_output_file(\n",
    "        'labels.pkl',\n",
    "        predictions,\n",
    "    )\n",
    "\n",
    "    ret['clusters'] = save_output_file(\n",
    "        'clusters.pkl',\n",
    "        list(set(predictions)),\n",
    "    )\n",
    "\n",
    "    \n",
    "    #############################################################################\n",
    "    # build classifier\n",
    "    logger.critical('ZSL - building classifier')\n",
    "    X_train = tensor_to_array(vectors)\n",
    "    y_train = predictions\n",
    "\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    classifier = KNeighborsClassifier(\n",
    "        algorithm='auto', leaf_size=30, metric='euclidean',\n",
    "        metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
    "        weights='distance')\n",
    "    classifier.fit(X_train, y_train)\n",
    "    ret['CLASSIFIER'] = save_output_file(\n",
    "        'classifier.pkl',\n",
    "        classifier,\n",
    "    )\n",
    "    \n",
    "    elapsed_time = datetime.datetime.now() - start_t\n",
    "    logger.critical('ZSL - finished, Elapsed time {}'.format(elapsed_time))\n",
    "\n",
    "    return ret\n",
    "\n",
    "def fsl(\n",
    "    data,\n",
    "    target_column_nm,\n",
    "    samples,\n",
    "    samples_target_column_nm,\n",
    "    samples_tag_column_nm,\n",
    "    related_stcs=None,\n",
    "):\n",
    "    ret = {}\n",
    "    logger.critical('Start FSL')\n",
    "\n",
    "    #############################################################################\n",
    "    # Load data and Vectorize\n",
    "    logger.critical('FSL - vectorizing texts')\n",
    "    texts = data[target_column_nm].values.tolist()\n",
    "    sentences = []\n",
    "    vectors = vectorize(texts)\n",
    "    for i in range(0, len(texts)):\n",
    "        sentence = Sentence(texts[i])\n",
    "        sentences.append(sentence)\n",
    "    ret['vectors'] = save_output_file(\n",
    "        'vectors.pkl',\n",
    "        vectors\n",
    "    )\n",
    "\n",
    "    logger.critical('ZSL - loading pre-trained model')\n",
    "    tars = TARSClassifier.load('resources/taggers/agnews_all/final-model.pt') # Load model\n",
    "\n",
    "    smpl_stcs = samples[samples_target_column_nm].values.tolist()\n",
    "    smpl_tags = samples[samples_tag_column_nm].values.tolist()\n",
    "\n",
    "    class_nm_list = list(set(smpl_tags))\n",
    "    # Split for Train and Test\n",
    "    #x_train,x_test,y_train,y_test = train_test_split(smpl_stcs, smpl_tags, test_size=0.2, stratify=smpl_tags, random_state=3,shuffle=True)\n",
    "    \n",
    "    ## TODO if related_stcs:\n",
    "    if related_stcs is None:\n",
    "        logger.critical('FSL - No related sentences, converting samples')\n",
    "        # convert samples\n",
    "        tr = []\n",
    "        for i in range(0, len(smpl_stcs)):\n",
    "            tr.append(Sentence(smpl_stcs[i]).add_label('fsl', smpl_tags[i]))\n",
    "\n",
    "        train = SentenceDataset(tr)\n",
    "        corpus = Corpus(train=train)\n",
    "\n",
    "        # make the model aware of the desired set of labels from the new corpus\n",
    "        tars.add_and_switch_to_new_task(\"fsl\", label_dictionary=corpus.make_label_dictionary()) \n",
    "        # initialize the text classifier trainer with corpus\n",
    "        trainer = ModelTrainer(tars, corpus)\n",
    "    \n",
    "        # train model\n",
    "        logger.critical('FSL - Training samples')\n",
    "        trainer.train(base_path='./resources/taggers/fsl', # path to store the model artifacts\n",
    "                      learning_rate=0.02, # use very small learning rate\n",
    "                      mini_batch_size=1, # small mini-batch size since corpus is tiny\n",
    "                      max_epochs=10, # terminate after 10 epochs\n",
    "                      train_with_dev=True,\n",
    "                      )\n",
    "    \n",
    "        logger.critical('FSL - Loading the trained model')\n",
    "        tars = TARSClassifier.load('./resources/taggers/fsl/final-model.pt') # Load model\n",
    "\n",
    "        logger.critical('FSL - Predicting using the trained model')\n",
    "        predictions = []\n",
    "        for sentence in sentences:\n",
    "            tars.predict_zero_shot(sentence, class_nm_list) # Predict zero-shot\n",
    "            number_list = [c['confidence'] for c in sentence.to_dict()['labels']]\n",
    "            if len(number_list) > 0:\n",
    "                max_value = max(number_list)\n",
    "                max_index = number_list.index(max_value)\n",
    "                predictions.append(sentence.to_dict()['labels'][max_index]['value'])\n",
    "            else :\n",
    "                predictions.append('분류못함')\n",
    "\n",
    "        ret['labels'] = save_output_file(\n",
    "            'labels.pkl',\n",
    "            predictions,\n",
    "        )\n",
    "\n",
    "        ret['clusters'] = save_output_file(\n",
    "            'clusters.pkl',\n",
    "            list(set(predictions)),\n",
    "        )\n",
    "\n",
    "        #############################################################################\n",
    "        # build classifier\n",
    "        logger.critical('FSL - building classifier')\n",
    "        X_train = tensor_to_array(vectors)\n",
    "        y_train = predictions\n",
    "\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        classifier = KNeighborsClassifier(\n",
    "            algorithm='auto', leaf_size=30, metric='euclidean',\n",
    "            metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
    "            weights='distance')\n",
    "        classifier.fit(X_train, y_train)\n",
    "        ret['CLASSIFIER'] = save_output_file(\n",
    "            'classifier.pkl',\n",
    "            classifier,\n",
    "        )\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n",
    "def main(confpath, output_path, logger):\n",
    "    if not os.path.isfile(confpath):\n",
    "        raise Exception('no conf file')\n",
    "    with open(confpath, 'rb') as f:\n",
    "        conf = pickle.load(f)\n",
    "\n",
    "    logger.critical(\n",
    "        {k: conf[k] for k in sorted(conf.keys())}\n",
    "    )\n",
    "\n",
    "    worker_type = conf['labeler_worker_type']\n",
    "\n",
    "    try:\n",
    "        file = conf['file']\n",
    "        target_column_nm = conf['target_column_nm']\n",
    "        class_nm_list = conf.get('class_nm_list', None)\n",
    "        if worker_type == 'zsl':\n",
    "\n",
    "            target_df = load(file)\n",
    "            d = zsl(\n",
    "                target_df,\n",
    "                target_column_nm,\n",
    "                class_nm_list,\n",
    "            )\n",
    "            (output_path / 'output.json').write_text(\n",
    "                json.dumps(d)\n",
    "            )\n",
    "\n",
    "        elif worker_type == 'fsl':\n",
    "            samples_fp = conf['samples_fp']\n",
    "            samples_target_column_nm = conf.get('samples_target_column_nm', target_column_nm)\n",
    "            samples_tag_column_nm = conf['samples_tag_column_nm']\n",
    "            related_stcs = conf.get('related_stcs', None)\n",
    "\n",
    "            target_df = load(file)\n",
    "            if samples_fp:\n",
    "                sample_df = load(samples_fp)\n",
    "            else:\n",
    "                sample_df = None\n",
    "            d = fsl(\n",
    "                target_df,\n",
    "                target_column_nm,\n",
    "                sample_df,\n",
    "                samples_target_column_nm,\n",
    "                samples_tag_column_nm,\n",
    "                related_stcs,\n",
    "            )\n",
    "            (output_path / 'output.json').write_text(\n",
    "                json.dumps(d)\n",
    "            )\n",
    "\n",
    "        elif worker_type == \"lb_predict\":\n",
    "            logger.critical('AccuTuning Labeler - lb_predict')\n",
    "            classifier_fp = conf['classifier_fp']\n",
    "            texts = conf['texts']\n",
    "            bulk = conf['bulk']\n",
    "\n",
    "            start_t = datetime.datetime.now()\n",
    "\n",
    "            workspace_home = Path(os.environ.get('ACCUTUNING_WORKSPACE_ROOT', '/workspace'))\n",
    "\n",
    "            classifier = pickle.loads((workspace_home / classifier_fp).read_bytes())\n",
    "\n",
    "            mid_t = datetime.datetime.now()\n",
    "\n",
    "            pred_list = []\n",
    "            vectors = vectorize(texts)\n",
    "            for vector in vectors:\n",
    "                if torch.is_tensor(vector):\n",
    "                    vector = np.array(vector.cpu()).reshape(1, -1)\n",
    "                int_pred = classifier.predict(vector)\n",
    "                pred_list.append(int_pred)\n",
    "\n",
    "            end_t = datetime.datetime.now()\n",
    "\n",
    "\n",
    "            if bulk:\n",
    "                bulk_output_fp = conf['bulk_output_fp']\n",
    "                pd.DataFrame({'input': texts, 'output': pred_list}).to_csv(bulk_output_fp, index=False)\n",
    "\n",
    "            output_path.write_bytes(\n",
    "                pickle.dumps(\n",
    "                    dict(\n",
    "                        pred=pred_list[0],\n",
    "                        total_duration=(end_t - start_t).microseconds,\n",
    "                        pred_duration=(end_t - mid_t).microseconds,\n",
    "                        bulk_output_fp=str(bulk_output_fp.relative_to(workspace_home)) if bulk else ''\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise Exception('unknown worker_type:' + worker_type)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(str(e))\n",
    "\n",
    "\n",
    "def evaluate(target_df, pred):\n",
    "    # stcs = target_df['stcs'].values.tolist()\n",
    "    tags = target_df['tags'].values.tolist()\n",
    "\n",
    "    print('분류 못함 ', dict(Counter(pred))['분류못함'], '건 포함 metrics')\n",
    "    print(metrics.classification_report(tags,pred))\n",
    "    print('')\n",
    "    pred_expt = []\n",
    "    tags_expt = []\n",
    "    for idx, p in enumerate(pred) :\n",
    "        if p == '분류못함' :\n",
    "            pass\n",
    "        else :\n",
    "            pred_expt.append(pred[idx])\n",
    "            tags_expt.append(tags[idx])\n",
    "    print('분류 못함 ', dict(Counter(pred))['분류못함'], '건 제외 metrics')\n",
    "    print(metrics.classification_report(tags_expt,pred_expt))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    workspace = os.environ.get('ACCUTUNING_WORKSPACE')\n",
    "    flagpath = os.path.join(workspace, 'flag')\n",
    "    if os.path.isdir(flagpath):\n",
    "        shutil.rmtree(flagpath)\n",
    "    os.makedirs(flagpath)\n",
    "    with open(os.path.join(workspace, 'flag', 'STARTED'), 'wt') as f:\n",
    "        f.write('')\n",
    "\n",
    "    ###############################################################\n",
    "    # build directories\n",
    "    intermediate_path = Path(workspace, 'intermediate')\n",
    "    output_path = Path(workspace, 'output')\n",
    "    intermediate_path.mkdir(exist_ok=True)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "\n",
    "    confpath = os.path.join(workspace, 'conf.pkl')\n",
    "\n",
    "    formatter = logging.Formatter('%(asctime)s %(levelname)s %(name)s - %(message)s')\n",
    "    ch = logging.FileHandler(\n",
    "        os.path.join(\n",
    "            workspace,\n",
    "            'labeling_fsl.log',\n",
    "        ),\n",
    "        mode='w'\n",
    "    )\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "    try:\n",
    "        if workspace is None:\n",
    "            raise Exception('no workspace env')\n",
    "        main(confpath, output_path, logger=logger)\n",
    "        with open(os.path.join(workspace, 'flag', 'DONE'), 'wt') as f:\n",
    "            f.write('')\n",
    "    except Exception as e:\n",
    "        logging.critical(e, exc_info=True)\n",
    "        with open(os.path.join(workspace, 'flag', 'ERROR'), 'wt') as f:\n",
    "            f.write(str(e))\n",
    "    finally:\n",
    "        with open(os.path.join(workspace, 'flag', 'FINISHED'), 'wt') as f:\n",
    "            f.write('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a46721b",
   "metadata": {},
   "source": [
    "## ZSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cb1f46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('sources/nnst_lt_1990.csv')\n",
    "target = 'stcs'\n",
    "cols = ['생활', '기술', '기타']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c8894b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:Start ZSL\n",
      "CRITICAL:root:ZSL - vectorizing texts\n",
      "CRITICAL:root:Saved output file - vectors.pkl\n",
      "CRITICAL:root:ZSL - loading pre-trained model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-21 01:37:10,284 loading file ./resources/taggers/agnews_all/final-model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:ZSL - predicting zero shot\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init TARS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:Saved output file - labels.pkl\n",
      "CRITICAL:root:Saved output file - clusters.pkl\n",
      "CRITICAL:root:ZSL - building classifier\n",
      "CRITICAL:root:Saved output file - classifier.pkl\n",
      "CRITICAL:root:ZSL - finished, Elapsed time 0:04:35.135375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vectors': 'output/vectors.pkl',\n",
       " 'labels': 'output/labels.pkl',\n",
       " 'clusters': 'output/clusters.pkl',\n",
       " 'CLASSIFIER': 'output/classifier.pkl'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zsl(data, target, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0b180d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = pickle.load(open('output/vectors.pkl', 'rb'))\n",
    "labels = pickle.load(open('output/labels.pkl', 'rb'))\n",
    "clusters = pickle.load(open('output/clusters.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b72c9d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.99578553, -0.00535922,  0.8316863 , ...,  0.19431722,\n",
       "         0.11335345, -0.61855346],\n",
       "       [-0.6068382 ,  0.12818483,  0.71900487, ...,  0.32662803,\n",
       "         0.03273866, -0.59235084],\n",
       "       [-0.80009407, -0.26659516,  0.8736665 , ...,  0.36003995,\n",
       "         0.21039687, -0.6081318 ],\n",
       "       ...,\n",
       "       [-0.67031485, -0.04569531,  0.9157012 , ..., -0.03234894,\n",
       "         0.3023741 , -0.47822633],\n",
       "       [-0.59436864, -0.18450913,  1.0091789 , ...,  0.07618729,\n",
       "         0.09041867, -0.26631007],\n",
       "       [-0.6379605 , -0.15387407,  0.83737415, ..., -0.12898003,\n",
       "        -0.0330771 , -0.38149998]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "653f7753",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '기술',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '기술',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '기술',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '기술',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '기술',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '기술',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '기술',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '기타',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '기술',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기타',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '분류못함',\n",
       " '기타',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '기술',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '기술',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기타',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '기술',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '기술',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '기술',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '분류못함',\n",
       " '기술',\n",
       " '생활',\n",
       " '분류못함',\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c291d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['분류못함', '생활', '기술', '기타']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7fcf1e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          기술       0.79      0.21      0.33       995\n",
      "          기타       0.00      0.00      0.00         0\n",
      "        분류못함       0.00      0.00      0.00         0\n",
      "          생활       0.78      0.63      0.70       995\n",
      "\n",
      "    accuracy                           0.42      1990\n",
      "   macro avg       0.39      0.21      0.26      1990\n",
      "weighted avg       0.78      0.42      0.52      1990\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(data['tags'], labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e4f6a9",
   "metadata": {},
   "source": [
    "## FSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9775e869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:Start FSL\n",
      "CRITICAL:root:FSL - vectorizing texts\n",
      "CRITICAL:root:Saved output file - vectors.pkl\n",
      "CRITICAL:root:ZSL - loading pre-trained model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-21 01:20:26,643 loading file resources/taggers/agnews_all/final-model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:FSL - No related sentences, converting samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init TARS\n",
      "2021-07-21 01:20:35,711 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 7368.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-21 01:20:35,716 [b'\\xec\\x83\\x9d\\xed\\x99\\x9c', b'\\xea\\xb8\\xb0\\xec\\x88\\xa0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "CRITICAL:root:FSL - Training samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-21 01:20:35,719 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-21 01:20:35,723 Model: \"TARSClassifier(\n",
      "  (document_embeddings): None\n",
      "  (decoder): None\n",
      "  (loss_function): None\n",
      "  (tars_model): TextClassifier(\n",
      "    (document_embeddings): TransformerDocumentEmbeddings(\n",
      "      (model): BertModel(\n",
      "        (embeddings): BertEmbeddings(\n",
      "          (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
      "          (position_embeddings): Embedding(512, 768)\n",
      "          (token_type_embeddings): Embedding(2, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): BertEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (6): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (7): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (8): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (9): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (10): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (11): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (pooler): BertPooler(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (activation): Tanh()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder): Linear(in_features=768, out_features=2, bias=True)\n",
      "    (loss_function): CrossEntropyLoss()\n",
      "  )\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2021-07-21 01:20:35,725 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-21 01:20:35,725 Corpus: \"Corpus: 8 train + 1 dev + 1 test sentences\"\n",
      "2021-07-21 01:20:35,726 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-21 01:20:35,727 Parameters:\n",
      "2021-07-21 01:20:35,728  - learning_rate: \"0.02\"\n",
      "2021-07-21 01:20:35,729  - mini_batch_size: \"1\"\n",
      "2021-07-21 01:20:35,730  - patience: \"3\"\n",
      "2021-07-21 01:20:35,731  - anneal_factor: \"0.5\"\n",
      "2021-07-21 01:20:35,731  - max_epochs: \"10\"\n",
      "2021-07-21 01:20:35,732  - shuffle: \"True\"\n",
      "2021-07-21 01:20:35,733  - train_with_dev: \"True\"\n",
      "2021-07-21 01:20:35,733  - batch_growth_annealing: \"False\"\n",
      "2021-07-21 01:20:35,734 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-21 01:20:35,735 Model training base path: \"resources/taggers/fsl\"\n",
      "2021-07-21 01:20:35,735 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-21 01:20:35,736 Device: cpu\n",
      "2021-07-21 01:20:35,737 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-21 01:20:35,737 Embeddings storage mode: cpu\n",
      "2021-07-21 01:20:35,743 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-21 01:20:36,385 epoch 1 - iter 1/9 - loss 1.31945038 - samples/sec: 2.22 - lr: 0.020000\n",
      "2021-07-21 01:20:36,778 epoch 1 - iter 2/9 - loss 1.00522080 - samples/sec: 2.55 - lr: 0.020000\n",
      "2021-07-21 01:20:37,226 epoch 1 - iter 3/9 - loss 0.82822293 - samples/sec: 2.24 - lr: 0.020000\n",
      "2021-07-21 01:20:37,651 epoch 1 - iter 4/9 - loss 0.65816759 - samples/sec: 2.37 - lr: 0.020000\n",
      "2021-07-21 01:20:38,128 epoch 1 - iter 5/9 - loss 1.13733472 - samples/sec: 2.10 - lr: 0.020000\n",
      "2021-07-21 01:20:38,847 epoch 1 - iter 6/9 - loss 0.98447693 - samples/sec: 1.39 - lr: 0.020000\n",
      "2021-07-21 01:20:39,313 epoch 1 - iter 7/9 - loss 0.85268459 - samples/sec: 2.15 - lr: 0.020000\n",
      "2021-07-21 01:20:39,868 epoch 1 - iter 8/9 - loss 0.74733236 - samples/sec: 1.81 - lr: 0.020000\n",
      "2021-07-21 01:20:40,310 epoch 1 - iter 9/9 - loss 0.98932189 - samples/sec: 2.28 - lr: 0.020000\n",
      "2021-07-21 01:20:40,313 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-21 01:20:40,314 EPOCH 1 done: loss 0.9893 - lr 0.0200000\n",
      "2021-07-21 01:20:40,315 BAD EPOCHS (no improvement): 0\n",
      "2021-07-21 01:20:40,317 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-21 01:20:40,879 epoch 2 - iter 1/9 - loss 0.03209624 - samples/sec: 2.07 - lr: 0.020000\n",
      "2021-07-21 01:20:41,279 epoch 2 - iter 2/9 - loss 0.04242722 - samples/sec: 2.52 - lr: 0.020000\n",
      "2021-07-21 01:20:41,809 epoch 2 - iter 3/9 - loss 0.03153669 - samples/sec: 1.89 - lr: 0.020000\n",
      "2021-07-21 01:20:42,237 epoch 2 - iter 4/9 - loss 0.02549539 - samples/sec: 2.35 - lr: 0.020000\n",
      "2021-07-21 01:20:42,973 epoch 2 - iter 5/9 - loss 0.02515225 - samples/sec: 1.36 - lr: 0.020000\n",
      "2021-07-21 01:20:43,582 epoch 2 - iter 6/9 - loss 0.02240924 - samples/sec: 1.65 - lr: 0.020000\n",
      "2021-07-21 01:20:44,166 epoch 2 - iter 7/9 - loss 0.02007190 - samples/sec: 1.72 - lr: 0.020000\n",
      "2021-07-21 01:20:44,661 epoch 2 - iter 8/9 - loss 0.01776831 - samples/sec: 2.03 - lr: 0.020000\n",
      "2021-07-21 01:20:45,069 epoch 2 - iter 9/9 - loss 0.01624622 - samples/sec: 2.47 - lr: 0.020000\n",
      "2021-07-21 01:20:45,074 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-21 01:20:45,076 EPOCH 2 done: loss 0.0162 - lr 0.0200000\n",
      "2021-07-21 01:20:45,077 BAD EPOCHS (no improvement): 0\n",
      "2021-07-21 01:20:45,079 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-21 01:20:45,561 epoch 3 - iter 1/9 - loss 0.00212660 - samples/sec: 2.43 - lr: 0.020000\n",
      "2021-07-21 01:20:45,964 epoch 3 - iter 2/9 - loss 0.00418325 - samples/sec: 2.49 - lr: 0.020000\n",
      "2021-07-21 01:20:46,383 epoch 3 - iter 3/9 - loss 0.00320183 - samples/sec: 2.40 - lr: 0.020000\n",
      "2021-07-21 01:20:46,879 epoch 3 - iter 4/9 - loss 0.00322066 - samples/sec: 2.02 - lr: 0.020000\n",
      "2021-07-21 01:20:47,362 epoch 3 - iter 5/9 - loss 0.00303555 - samples/sec: 2.07 - lr: 0.020000\n",
      "2021-07-21 01:20:47,895 epoch 3 - iter 6/9 - loss 0.00319026 - samples/sec: 1.89 - lr: 0.020000\n",
      "2021-07-21 01:20:48,357 epoch 3 - iter 7/9 - loss 0.00281597 - samples/sec: 2.17 - lr: 0.020000\n",
      "2021-07-21 01:20:48,815 epoch 3 - iter 8/9 - loss 0.00264465 - samples/sec: 2.19 - lr: 0.020000\n",
      "2021-07-21 01:20:49,242 epoch 3 - iter 9/9 - loss 0.00247076 - samples/sec: 2.35 - lr: 0.020000\n",
      "2021-07-21 01:20:49,245 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-21 01:20:49,245 EPOCH 3 done: loss 0.0025 - lr 0.0200000\n",
      "2021-07-21 01:20:49,246 BAD EPOCHS (no improvement): 0\n",
      "2021-07-21 01:20:49,248 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-21 01:20:49,825 epoch 4 - iter 1/9 - loss 0.00094408 - samples/sec: 1.94 - lr: 0.020000\n",
      "2021-07-21 01:20:50,415 epoch 4 - iter 2/9 - loss 0.00206684 - samples/sec: 1.70 - lr: 0.020000\n",
      "2021-07-21 01:20:50,892 epoch 4 - iter 3/9 - loss 0.00166825 - samples/sec: 2.10 - lr: 0.020000\n",
      "2021-07-21 01:20:51,315 epoch 4 - iter 4/9 - loss 0.00136752 - samples/sec: 2.37 - lr: 0.020000\n",
      "2021-07-21 01:20:51,703 epoch 4 - iter 5/9 - loss 0.00164823 - samples/sec: 2.59 - lr: 0.020000\n",
      "2021-07-21 01:20:52,096 epoch 4 - iter 6/9 - loss 0.00164561 - samples/sec: 2.56 - lr: 0.020000\n",
      "2021-07-21 01:20:52,541 epoch 4 - iter 7/9 - loss 0.00233378 - samples/sec: 2.25 - lr: 0.020000\n",
      "2021-07-21 01:20:53,065 epoch 4 - iter 8/9 - loss 0.00211122 - samples/sec: 1.91 - lr: 0.020000\n",
      "2021-07-21 01:20:53,598 epoch 4 - iter 9/9 - loss 0.00213556 - samples/sec: 1.88 - lr: 0.020000\n",
      "2021-07-21 01:20:53,601 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-21 01:20:53,602 EPOCH 4 done: loss 0.0021 - lr 0.0200000\n",
      "2021-07-21 01:20:53,603 BAD EPOCHS (no improvement): 0\n",
      "2021-07-21 01:20:53,604 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-21 01:20:54,325 epoch 5 - iter 1/9 - loss 0.00268673 - samples/sec: 1.50 - lr: 0.020000\n",
      "2021-07-21 01:20:54,777 epoch 5 - iter 2/9 - loss 0.00208501 - samples/sec: 2.22 - lr: 0.020000\n",
      "2021-07-21 01:20:55,241 epoch 5 - iter 3/9 - loss 0.00156549 - samples/sec: 2.16 - lr: 0.020000\n",
      "2021-07-21 01:20:55,661 epoch 5 - iter 4/9 - loss 0.00148706 - samples/sec: 2.40 - lr: 0.020000\n",
      "2021-07-21 01:20:56,201 epoch 5 - iter 5/9 - loss 0.00146093 - samples/sec: 1.86 - lr: 0.020000\n",
      "2021-07-21 01:20:57,214 epoch 5 - iter 6/9 - loss 0.00129864 - samples/sec: 0.99 - lr: 0.020000\n",
      "2021-07-21 01:20:57,615 epoch 5 - iter 7/9 - loss 0.00133615 - samples/sec: 2.51 - lr: 0.020000\n",
      "2021-07-21 01:20:58,014 epoch 5 - iter 8/9 - loss 0.00129699 - samples/sec: 2.52 - lr: 0.020000\n",
      "2021-07-21 01:20:58,453 epoch 5 - iter 9/9 - loss 0.00143637 - samples/sec: 2.28 - lr: 0.020000\n",
      "2021-07-21 01:20:58,459 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-21 01:20:58,460 EPOCH 5 done: loss 0.0014 - lr 0.0200000\n",
      "2021-07-21 01:20:58,461 BAD EPOCHS (no improvement): 0\n",
      "2021-07-21 01:20:58,464 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-21 01:20:58,940 epoch 6 - iter 1/9 - loss 0.00181708 - samples/sec: 2.41 - lr: 0.020000\n",
      "2021-07-21 01:20:59,388 epoch 6 - iter 2/9 - loss 0.00124808 - samples/sec: 2.24 - lr: 0.020000\n",
      "2021-07-21 01:20:59,778 epoch 6 - iter 3/9 - loss 0.00126940 - samples/sec: 2.57 - lr: 0.020000\n",
      "2021-07-21 01:21:00,197 epoch 6 - iter 4/9 - loss 0.00117595 - samples/sec: 2.40 - lr: 0.020000\n",
      "2021-07-21 01:21:00,554 epoch 6 - iter 5/9 - loss 0.00106313 - samples/sec: 2.82 - lr: 0.020000\n",
      "2021-07-21 01:21:01,162 epoch 6 - iter 6/9 - loss 0.00107563 - samples/sec: 1.65 - lr: 0.020000\n",
      "2021-07-21 01:21:01,689 epoch 6 - iter 7/9 - loss 0.00105007 - samples/sec: 1.90 - lr: 0.020000\n",
      "2021-07-21 01:21:02,114 epoch 6 - iter 8/9 - loss 0.00096282 - samples/sec: 2.36 - lr: 0.020000\n",
      "2021-07-21 01:21:02,542 epoch 6 - iter 9/9 - loss 0.00089978 - samples/sec: 2.36 - lr: 0.020000\n",
      "2021-07-21 01:21:02,545 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-21 01:21:02,545 EPOCH 6 done: loss 0.0009 - lr 0.0200000\n",
      "2021-07-21 01:21:02,546 BAD EPOCHS (no improvement): 0\n",
      "2021-07-21 01:21:02,547 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-21 01:21:02,971 epoch 7 - iter 1/9 - loss 0.00056227 - samples/sec: 2.60 - lr: 0.020000\n",
      "2021-07-21 01:21:03,403 epoch 7 - iter 2/9 - loss 0.00163847 - samples/sec: 2.32 - lr: 0.020000\n",
      "2021-07-21 01:21:04,146 epoch 7 - iter 3/9 - loss 0.00145574 - samples/sec: 1.35 - lr: 0.020000\n",
      "2021-07-21 01:21:04,584 epoch 7 - iter 4/9 - loss 0.00121933 - samples/sec: 2.29 - lr: 0.020000\n",
      "2021-07-21 01:21:05,119 epoch 7 - iter 5/9 - loss 0.00113775 - samples/sec: 1.87 - lr: 0.020000\n",
      "2021-07-21 01:21:05,553 epoch 7 - iter 6/9 - loss 0.00099681 - samples/sec: 2.31 - lr: 0.020000\n",
      "2021-07-21 01:21:05,960 epoch 7 - iter 7/9 - loss 0.00090209 - samples/sec: 2.47 - lr: 0.020000\n",
      "2021-07-21 01:21:06,365 epoch 7 - iter 8/9 - loss 0.00110867 - samples/sec: 2.48 - lr: 0.020000\n",
      "2021-07-21 01:21:06,753 epoch 7 - iter 9/9 - loss 0.00103284 - samples/sec: 2.58 - lr: 0.020000\n",
      "2021-07-21 01:21:06,757 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-21 01:21:06,758 EPOCH 7 done: loss 0.0010 - lr 0.0200000\n",
      "2021-07-21 01:21:06,758 BAD EPOCHS (no improvement): 1\n",
      "2021-07-21 01:21:06,760 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-21 01:21:07,322 epoch 8 - iter 1/9 - loss 0.00152006 - samples/sec: 2.30 - lr: 0.020000\n",
      "2021-07-21 01:21:07,755 epoch 8 - iter 2/9 - loss 0.00207064 - samples/sec: 2.32 - lr: 0.020000\n",
      "2021-07-21 01:21:08,189 epoch 8 - iter 3/9 - loss 0.00154100 - samples/sec: 2.31 - lr: 0.020000\n",
      "2021-07-21 01:21:08,726 epoch 8 - iter 4/9 - loss 0.00130049 - samples/sec: 1.87 - lr: 0.020000\n",
      "2021-07-21 01:21:09,176 epoch 8 - iter 5/9 - loss 0.00113065 - samples/sec: 2.23 - lr: 0.020000\n",
      "2021-07-21 01:21:09,689 epoch 8 - iter 6/9 - loss 0.00126843 - samples/sec: 1.96 - lr: 0.020000\n",
      "2021-07-21 01:21:10,355 epoch 8 - iter 7/9 - loss 0.00125701 - samples/sec: 1.51 - lr: 0.020000\n",
      "2021-07-21 01:21:10,782 epoch 8 - iter 8/9 - loss 0.00114547 - samples/sec: 2.35 - lr: 0.020000\n",
      "2021-07-21 01:21:11,237 epoch 8 - iter 9/9 - loss 0.00106777 - samples/sec: 2.21 - lr: 0.020000\n",
      "2021-07-21 01:21:11,242 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-21 01:21:11,243 EPOCH 8 done: loss 0.0011 - lr 0.0200000\n",
      "2021-07-21 01:21:11,244 BAD EPOCHS (no improvement): 2\n",
      "2021-07-21 01:21:11,246 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-21 01:21:11,740 epoch 9 - iter 1/9 - loss 0.00042024 - samples/sec: 2.68 - lr: 0.020000\n",
      "2021-07-21 01:21:12,119 epoch 9 - iter 2/9 - loss 0.00096012 - samples/sec: 2.65 - lr: 0.020000\n",
      "2021-07-21 01:21:13,033 epoch 9 - iter 3/9 - loss 0.00086905 - samples/sec: 1.10 - lr: 0.020000\n",
      "2021-07-21 01:21:13,459 epoch 9 - iter 4/9 - loss 0.00076132 - samples/sec: 2.35 - lr: 0.020000\n",
      "2021-07-21 01:21:13,990 epoch 9 - iter 5/9 - loss 0.00073006 - samples/sec: 1.89 - lr: 0.020000\n",
      "2021-07-21 01:21:14,394 epoch 9 - iter 6/9 - loss 0.00068102 - samples/sec: 2.48 - lr: 0.020000\n",
      "2021-07-21 01:21:14,942 epoch 9 - iter 7/9 - loss 0.00073330 - samples/sec: 1.83 - lr: 0.020000\n",
      "2021-07-21 01:21:15,386 epoch 9 - iter 8/9 - loss 0.00067134 - samples/sec: 2.26 - lr: 0.020000\n",
      "2021-07-21 01:21:15,779 epoch 9 - iter 9/9 - loss 0.00064269 - samples/sec: 2.56 - lr: 0.020000\n",
      "2021-07-21 01:21:15,784 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-21 01:21:15,785 EPOCH 9 done: loss 0.0006 - lr 0.0200000\n",
      "2021-07-21 01:21:15,786 BAD EPOCHS (no improvement): 0\n",
      "2021-07-21 01:21:15,788 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-21 01:21:16,229 epoch 10 - iter 1/9 - loss 0.00036108 - samples/sec: 2.69 - lr: 0.020000\n",
      "2021-07-21 01:21:16,759 epoch 10 - iter 2/9 - loss 0.00051799 - samples/sec: 1.89 - lr: 0.020000\n",
      "2021-07-21 01:21:17,190 epoch 10 - iter 3/9 - loss 0.00045472 - samples/sec: 2.33 - lr: 0.020000\n",
      "2021-07-21 01:21:17,625 epoch 10 - iter 4/9 - loss 0.00062186 - samples/sec: 2.31 - lr: 0.020000\n",
      "2021-07-21 01:21:18,043 epoch 10 - iter 5/9 - loss 0.00056018 - samples/sec: 2.40 - lr: 0.020000\n",
      "2021-07-21 01:21:18,483 epoch 10 - iter 6/9 - loss 0.00049514 - samples/sec: 2.28 - lr: 0.020000\n",
      "2021-07-21 01:21:19,221 epoch 10 - iter 7/9 - loss 0.00051562 - samples/sec: 1.36 - lr: 0.020000\n",
      "2021-07-21 01:21:19,779 epoch 10 - iter 8/9 - loss 0.00048533 - samples/sec: 1.80 - lr: 0.020000\n",
      "2021-07-21 01:21:20,718 epoch 10 - iter 9/9 - loss 0.00059170 - samples/sec: 1.07 - lr: 0.020000\n",
      "2021-07-21 01:21:20,722 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-21 01:21:20,724 EPOCH 10 done: loss 0.0006 - lr 0.0200000\n",
      "2021-07-21 01:21:20,724 BAD EPOCHS (no improvement): 0\n",
      "2021-07-21 01:21:22,505 ----------------------------------------------------------------------------------------------------\n",
      "2021-07-21 01:21:22,507 Testing using best model ...\n",
      "2021-07-21 01:21:22,712 \t1.0\n",
      "2021-07-21 01:21:22,713 \n",
      "Results:\n",
      "- F-score (micro) 1.0\n",
      "- F-score (macro) 0.5\n",
      "- Accuracy 1.0\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          생활     0.0000    0.0000    0.0000         0\n",
      "          기술     1.0000    1.0000    1.0000         1\n",
      "\n",
      "   micro avg     1.0000    1.0000    1.0000         1\n",
      "   macro avg     0.5000    0.5000    0.5000         1\n",
      "weighted avg     1.0000    1.0000    1.0000         1\n",
      " samples avg     1.0000    1.0000    1.0000         1\n",
      "\n",
      "2021-07-21 01:21:22,715 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:FSL - Loading the trained model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-21 01:21:22,720 loading file ./resources/taggers/fsl/final-model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:FSL - Predicting using the trained model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init TARS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:Saved output file - labels.pkl\n",
      "CRITICAL:root:Saved output file - clusters.pkl\n",
      "CRITICAL:root:FSL - building classifier\n",
      "CRITICAL:root:Saved output file - classifier.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vectors': 'output/vectors.pkl',\n",
       " 'labels': 'output/labels.pkl',\n",
       " 'clusters': 'output/clusters.pkl',\n",
       " 'CLASSIFIER': 'output/classifier.pkl'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = pd.read_csv('sources/nnst_lt_10.csv')\n",
    "samples_target_column_nm = 'stcs'\n",
    "samples_tag_column_nm = 'tags'\n",
    "\n",
    "fsl(data, target, samples, samples_target_column_nm, samples_tag_column_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1813bdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = pickle.load(open('output/vectors.pkl', 'rb'))\n",
    "labels = pickle.load(open('output/labels.pkl', 'rb'))\n",
    "clusters = pickle.load(open('output/clusters.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27b7a496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1990, 768)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8b01745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '분류못함',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '기술',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '분류못함',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '기술',\n",
       " '기술',\n",
       " '기술',\n",
       " '생활',\n",
       " '기술',\n",
       " ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "526ab60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['기술', '생활', '분류못함']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96ad3c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          기술       0.80      0.74      0.77       995\n",
      "        분류못함       0.00      0.00      0.00         0\n",
      "          생활       0.77      0.81      0.79       995\n",
      "\n",
      "    accuracy                           0.77      1990\n",
      "   macro avg       0.52      0.52      0.52      1990\n",
      "weighted avg       0.78      0.77      0.78      1990\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(data['tags'], labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe163569",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
